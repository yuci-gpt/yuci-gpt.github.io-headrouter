<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HeadRouter: A Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a>Yu Xu<sup>1</sup></a>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=PdKElfwAAAAJ" target="_blank">Fan Tang<sup>1</sup></a>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fSBdNg0AAAAJ&hl=zh-CN&oi=ao" target="_blank">Juan Cao<sup>1</sup></a>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=8VD0_DkAAAAJ" target="_blank">Yuxin Zhang<sup>2</sup></a>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=28FH5F0AAAAJ&hl=zh-CN&oi=ao" target="_blank">Xiaoyu Kong<sup>3</sup></a>,</span>
                </span>
                <span class="author-block">
                <a>Jintao Li<sup>1</sup></a>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=y3j0c80AAAAJ" target="_blank">Oliver Deussen<sup>4</sup></a></span>
                <span class="author-block">
                <a href="http://graphics.csie.ncku.edu.tw/Tony/tony.htm" target="_blank">Tong-Yee Lee<sup>5</sup></a></span>
                </div>
                
                <div class="is-size-5 publication-authors"></div>
                <span class="author-block">
                    <sup>1</sup>Institute of Computing Technology, Chinese Academy of Sciences<br>
                    <sup>2</sup>Institute of Automation, Chinese Academy of Sciences<br>
                    <sup>3</sup>Beihang University<br>
                    <sup>4</sup>University of Konstanz<br>
                    <sup>5</sup>National Cheng-Kung University
                  </span>
                <!-- <span class="author-block"><sup>1</sup>Institute of Computing Technology, Chinese Academy of Sciences, China<br><sup>2</sup>AI Lab, Tencent, China<br><sup>3</sup>National Cheng Kung University, Taiwan</span> -->
                <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                </div>

                  <!-- Paper link -->
                  <!-- <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://dl.acm.org/doi/10.1145/3676165" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ICTMCG/HeadRouter" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Results of HeadRouter demonstrate accurate text-guided semantic representation while preserving consistency with the source image across diverse editing tasks.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    
    <!-- <div class="container is-max-desktop"> -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion Transformers (DiTs) have exhibited robust capabilities in image generation tasks. 
However, accurate text-guided image editing for multimodal DiTs (MM-DiTs) still poses a significant challenge.
            Unlike UNet-based structures that could utilize self/cross-attention maps for semantic editing, MM-DiTs inherently lack support for explicit and consistent incorporated text guidance, resulting in semantic misalignment between the edited results and texts. 
            In this study, we disclose the sensitivity of different attention heads to different image semantics within MM-DiTs and introduce \sysname, a training-free image editing framework that edits the source image by adaptively routing the text guidance to different attention heads in MM-DiTs.
            Furthermore, we present a dual-token refinement module to refine text/image token representations for precise semantic guidance and accurate region expression. 
            Experimental results on multiple benchmarks demonstrate HeadRouter's performance in terms of editing fidelity and image quality.
          </p>
        </div>
      </div>
    <!-- </div> -->
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">

      <h2 class="title is-3">Method</h2>
      <div class="item">
        <img src="static/images/pipeline.png" alt="MY ALT TEXT"/>
        <h2 class="content has-text-justified">
          Our method based on two key insights:
          (a) The various image semantics are adaptively distributed across different heads for MM-DiTs.
          (b) We identify and extract critical regions in the joint self-attention map where text tokens influence image tokens.
          In light of these observations, we present HeadRouter, a training-free image editing framework for MM-DiTs. An instance-adaptive attention head router (IARouter) is put forward, which adaptively activates attention heads based on their semantic sensitivity, thereby enabling a more accurate representation of the edited specific images.
          We further propose a dual-token refinement module (DTR) that employs self-enhancement of image tokens and rectified text token methods to enhance the representation of text-guided editing features in key regions and deep joint self-attention blocks. 
        </h2>
      </div>
    </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Image Results</h2>

      <h3 class="title is-4">Comparisons with baselines</h3>
      <div class="item">
        <img src="static/images/main_compare.png" alt="MY ALT TEXT"/>
      </div>

      <br>

      <!-- <h3 class="title is-4">Comparisons on More Data</h3> -->
      <!-- <div class="item">
        <img src="static/images/more_png.png" alt="MY ALT TEXT"/>
      </div> -->

      <br>
      
      <!-- <h3 class="title is-4">Swappable Attribute Customization</h3>
      <div class="item">
        <img src="static/images/adaptation_edit_png.png" alt="MY ALT TEXT"/>
      </div> -->

      <br>

      <!-- <h3 class="title is-4">Cross-domain Results</h3>
      <div class="item">
        <img src="static/images/cross_png.png" alt="MY ALT TEXT"/>
      </div> -->

      <br>

      <!-- <h3 class="title is-4">Re-aging</h3>
      <div class="item">
        <img src="static/images/reaging_png.png" alt="MY ALT TEXT"/>
      </div> -->

    </div>

  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Results</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/jYVJHFWT_YA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/cscs.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cscs,
        title={Identity-Preserving Face Swapping via Dual Surrogate Generative Models},
        author={Huang, Ziyao and Tang, Fan and Zhang, Yong and Cao, Juan and Li, Chengyu and Tang, Sheng and Li, Jintao and Lee, Tong-Yee},
        journal={ACM Transactions on Graphics (ToG)},
      }</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
